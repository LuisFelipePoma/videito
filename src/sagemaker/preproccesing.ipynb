{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877f354",
   "metadata": {},
   "source": [
    "# 1. Get Train Labels csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95011083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8925, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('AllLabels.csv')\n",
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbcba7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar Labels del CSV\n",
    "def load_labels(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    labels_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        clip_id = row['ClipID'].replace('.avi', '').replace('.mp4', '')\n",
    "        labels_dict[clip_id] = [row['Boredom'], row['Engagement'], row['Confusion'], row['Frustration']]\n",
    "    return labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644bc378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar dataset\n",
    "def process_dataset(frames_root_folder):\n",
    "    frames_root_folder = Path(frames_root_folder)\n",
    "    video_folders = list(frames_root_folder.glob(\"*/*\"))\n",
    "    with open(\"video_folders.txt\", \"w\") as f:\n",
    "        for folder in video_folders:\n",
    "            folder = str(folder).split(\"\\\\\")[-1]\n",
    "            f.write(str(folder) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ba9c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_root = \"output_frames/Train\"\n",
    "process_dataset(frames_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d1d8bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5481, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file with ClipIDs\n",
    "with open(\"video_folders.txt\", \"r\") as f:\n",
    "    clip_ids = [line.strip() for line in f]\n",
    "df['ClipID'] = df['ClipID'].str.replace('.avi', '').str.replace('.mp4', '')\n",
    "filtered_df = df[df['ClipID'].isin(clip_ids)]\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered_df to a csv file\n",
    "filtered_df.to_csv(\"Labels/TrainLabels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93839d17",
   "metadata": {},
   "source": [
    "# 2. Get Frames per Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19edf53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_video(video_path, output_folder, max_frames=None):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frame_count = 0\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    while success:\n",
    "        if max_frames and frame_count >= max_frames:\n",
    "            break\n",
    "        frame_filename = output_folder / f\"frame_{frame_count:04d}.jpg\"\n",
    "        cv2.imwrite(str(frame_filename), frame)\n",
    "        success, frame = cap.read()\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "\n",
    "def get_all_video_paths(dataset_dir, subset=\"Train\"):\n",
    "    dataset_path = Path(dataset_dir) / subset\n",
    "    video_paths = []\n",
    "    for person_folder in dataset_path.iterdir():\n",
    "        if person_folder.is_dir():\n",
    "            for video_folder in person_folder.iterdir():\n",
    "                if video_folder.is_dir():\n",
    "                    for video_file in video_folder.glob(\"*.*\"):\n",
    "                        if video_file.suffix.lower() in [\".avi\", \".mp4\"]:\n",
    "                            video_paths.append(video_file)\n",
    "\n",
    "    return video_paths\n",
    "\n",
    "def getFramesPerVideo(dataset_dir, subset=\"Train\",max_frames_per_video=None, output_base=\"output_frames\"):\n",
    "    video_paths = get_all_video_paths(dataset_dir, subset=subset)\n",
    "    print(f\"Procesando {len(video_paths)} videos del conjunto {subset}...\")\n",
    "\n",
    "    for video_path in tqdm(video_paths, desc=\"Extrayendo frames\"):\n",
    "        relative_path = video_path.relative_to(dataset_dir)\n",
    "        output_folder = Path(output_base) / relative_path.parent\n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "        extract_frames_from_video(video_path, output_folder, max_frames=max_frames_per_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "420c0645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 5481 videos del conjunto Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo frames: 100%|██████████| 5481/5481 [1:03:58<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "subset = \"Train\"  # Cambia a \"Test\" o \"Validation\" según sea necesario\n",
    "getFramesPerVideo(\n",
    "    dataset_dir=\"../Datasets/DaiSee/DAiSEE/DataSet/\",          # Ruta raíz al dataset DAiSEE\n",
    "    subset=subset,\n",
    "    #max_videos=6000,             # Cuántos videos procesar\n",
    "    max_frames_per_video=75,       # Frames máximos por video (None = todos)\n",
    "    output_base=\"output_frames\"   # Carpeta donde guardar los frames extraídos\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6b10d",
   "metadata": {},
   "source": [
    "# 3. Only Engagement Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4599fd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ClipID  Boredom  Engagement  Confusion  Frustration\n",
      "0  1100011002        0           2          0            0\n",
      "1  1100011003        0           2          0            0\n",
      "2  1100011004        0           3          0            0\n",
      "3  1100011005        0           3          0            0\n",
      "4  1100011006        0           3          0            0\n",
      "(5481, 5)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../Labels/TrainLabels.csv\")\n",
    "print(df_train.head())\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cc8445e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1100021003',\n",
       " '1100021055',\n",
       " '1100022005',\n",
       " '1100042023',\n",
       " '1100042026',\n",
       " '1100051007',\n",
       " '1100051016',\n",
       " '1100051030',\n",
       " '1100051031',\n",
       " '1100051053',\n",
       " '1100052014',\n",
       " '1100062008',\n",
       " '1100062045',\n",
       " '1100062049',\n",
       " '1100112002',\n",
       " '1100112006',\n",
       " '1100122056',\n",
       " '1100131017',\n",
       " '1100141013',\n",
       " '1100141027',\n",
       " '1100142033',\n",
       " '1100151011',\n",
       " '1100151057',\n",
       " '1100152010',\n",
       " '1100152017',\n",
       " '1100152031',\n",
       " '1100152055',\n",
       " '1100152070',\n",
       " '1100161053',\n",
       " '1100162005',\n",
       " '1100162016',\n",
       " '1100171004',\n",
       " '1100171008',\n",
       " '1100171059',\n",
       " '1100172012',\n",
       " '1100172017',\n",
       " '1100172033',\n",
       " '1100172034',\n",
       " '1100172043',\n",
       " '1100172058',\n",
       " '1100412018',\n",
       " '1100412033',\n",
       " '1100412039',\n",
       " '1110031010',\n",
       " '1110031025',\n",
       " '1110031027',\n",
       " '1110031033',\n",
       " '1110031038',\n",
       " '1110031056',\n",
       " '1110031063',\n",
       " '1110032014',\n",
       " '1110032027',\n",
       " '1110032043',\n",
       " '1813740138',\n",
       " '1813740184',\n",
       " '1813740185',\n",
       " '2000491077',\n",
       " '2000501006',\n",
       " '2000501030',\n",
       " '2000502053',\n",
       " '2000502065',\n",
       " '2000502081',\n",
       " '2026140257',\n",
       " '2026140264',\n",
       " '2026140273',\n",
       " '2056010134',\n",
       " '2056010224',\n",
       " '2100511069',\n",
       " '2100512051',\n",
       " '2100512064',\n",
       " '2100521032',\n",
       " '2100531054',\n",
       " '2100532016',\n",
       " '2100532022',\n",
       " '2100551005',\n",
       " '2100551010',\n",
       " '2100551032',\n",
       " '2100551035',\n",
       " '2100551042',\n",
       " '2100552048',\n",
       " '2100552063',\n",
       " '2100562019',\n",
       " '2100562024',\n",
       " '2100571002',\n",
       " '2100571007',\n",
       " '2100571021',\n",
       " '2100571023',\n",
       " '2100571038',\n",
       " '2100571044',\n",
       " '2100571049',\n",
       " '2100571062',\n",
       " '2100572021',\n",
       " '2100572057',\n",
       " '2100581001',\n",
       " '2100581021',\n",
       " '2100582027',\n",
       " '2100582052',\n",
       " '2100582055',\n",
       " '2100582056',\n",
       " '2100582057',\n",
       " '2100582058',\n",
       " '2100582060',\n",
       " '2100582061',\n",
       " '2100582062',\n",
       " '2100591046',\n",
       " '2100601012',\n",
       " '2100601018',\n",
       " '2100602041',\n",
       " '2100611027',\n",
       " '2100612009',\n",
       " '2260510155',\n",
       " '2260510163',\n",
       " '2260510177',\n",
       " '2408460118',\n",
       " '2408460137',\n",
       " '29042801370',\n",
       " '2904280260',\n",
       " '303830113',\n",
       " '303830126',\n",
       " '303830139',\n",
       " '303830149',\n",
       " '303830155',\n",
       " '30383023',\n",
       " '3100621037',\n",
       " '3100641003',\n",
       " '3100641004',\n",
       " '3100641006',\n",
       " '3100641023',\n",
       " '3100642007',\n",
       " '3100642017',\n",
       " '3100642036',\n",
       " '3100642055',\n",
       " '3100662037',\n",
       " '3100681015',\n",
       " '3100681017',\n",
       " '3100681018',\n",
       " '3100681042',\n",
       " '3100692015',\n",
       " '3100701010',\n",
       " '3100701023',\n",
       " '3100702038',\n",
       " '3100721031',\n",
       " '3100741066',\n",
       " '3100742058',\n",
       " '3100751006',\n",
       " '3100751007',\n",
       " '3100751008',\n",
       " '3100751010',\n",
       " '3100751012',\n",
       " '3100751019',\n",
       " '3100751056',\n",
       " '3100751057',\n",
       " '3100752014',\n",
       " '3100752021',\n",
       " '3100752030',\n",
       " '3100752037',\n",
       " '3100752048',\n",
       " '3100752051',\n",
       " '3100752055',\n",
       " '3100761003',\n",
       " '3100761005',\n",
       " '3100761008',\n",
       " '3100762004',\n",
       " '3100762027',\n",
       " '3100762030',\n",
       " '3100762055',\n",
       " '3100771001',\n",
       " '3100771007',\n",
       " '3100772051',\n",
       " '3100781002',\n",
       " '3100781007',\n",
       " '3100781034',\n",
       " '3100782067',\n",
       " '3100791058',\n",
       " '3100792045',\n",
       " '3100812028',\n",
       " '3100821022',\n",
       " '3100821032',\n",
       " '3100821033',\n",
       " '3100821038',\n",
       " '3100821048',\n",
       " '3100821052',\n",
       " '3100821057',\n",
       " '3100821069',\n",
       " '3100821075',\n",
       " '3100822011',\n",
       " '3100822031',\n",
       " '3100822065',\n",
       " '3100822066',\n",
       " '3344630121',\n",
       " '3344630131',\n",
       " '3344630162',\n",
       " '3344630211',\n",
       " '3344630282',\n",
       " '3422270111',\n",
       " '3422270129',\n",
       " '3422270167',\n",
       " '3503610158',\n",
       " '3503610168',\n",
       " '350361028',\n",
       " '4000181004',\n",
       " '4000181015',\n",
       " '4000182035',\n",
       " '401835015',\n",
       " '4018350282',\n",
       " '4110211001',\n",
       " '4110211004',\n",
       " '4110211005',\n",
       " '4110211013',\n",
       " '4110211014',\n",
       " '4110211015',\n",
       " '4110211021',\n",
       " '4110211022',\n",
       " '4110211023',\n",
       " '4110211025',\n",
       " '4110211027',\n",
       " '4110211028',\n",
       " '4110211036',\n",
       " '4110211038',\n",
       " '4110211039',\n",
       " '4110211040',\n",
       " '4110211045',\n",
       " '4110211053',\n",
       " '4110211055',\n",
       " '4110211061',\n",
       " '4110211075',\n",
       " '4110211078',\n",
       " '4110212034',\n",
       " '4110212036',\n",
       " '4110212049',\n",
       " '4110212051',\n",
       " '4110212062',\n",
       " '4110311006',\n",
       " '4110311023',\n",
       " '4110311045',\n",
       " '4110311048',\n",
       " '4110311054',\n",
       " '4110311062',\n",
       " '4110312009',\n",
       " '4140810153',\n",
       " '4140810162',\n",
       " '4140810210',\n",
       " '4140810217',\n",
       " '4140810219',\n",
       " '4140810272',\n",
       " '4599990132',\n",
       " '4599990235',\n",
       " '5221290280'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtra los ClipIDs con engagement == 0\n",
    "df_train_filtered = df_train.drop(columns=[\"Boredom\", \"Confusion\", \"Frustration\"])\n",
    "df_train_filtered['Engagement'] = df_train_filtered['Engagement'].replace({0: 0, 1: 0, 2: 1, 3: 1})\n",
    "df_train_filtered = df_train_filtered[df_train_filtered['Engagement'] == 0]\n",
    "\n",
    "# Set con los ClipID válidos\n",
    "valid_clip_ids = set(df_train_filtered['ClipID'].astype(str))\n",
    "valid_clip_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26bab06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_clip_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff109979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_frames_from_video(video_path, output_folder, max_frames=None):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frame_count = 0\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    while success:\n",
    "        if max_frames and frame_count >= max_frames:\n",
    "            break\n",
    "        frame_filename = output_folder / f\"frame_{frame_count:04d}.jpg\"\n",
    "        cv2.imwrite(str(frame_filename), frame)\n",
    "        success, frame = cap.read()\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "\n",
    "def get_all_video_paths(dataset_dir, subset=\"Train\"):\n",
    "    dataset_path = Path(dataset_dir) / subset\n",
    "    video_paths = []\n",
    "    for person_folder in dataset_path.iterdir():\n",
    "        if person_folder.is_dir():\n",
    "            for video_folder in person_folder.iterdir():\n",
    "                if video_folder.is_dir():\n",
    "                    for video_file in video_folder.glob(\"*.*\"):\n",
    "                        if video_file.suffix.lower() in [\".avi\", \".mp4\"]:\n",
    "                            video_paths.append(video_file)\n",
    "    return video_paths\n",
    "\n",
    "def getFramesPerVideo_filtered(dataset_dir, valid_clip_ids, subset=\"Train\", max_frames_per_video=None, output_base=\"Train_Augmentation\"):\n",
    "    video_paths = get_all_video_paths(dataset_dir, subset=subset)\n",
    "    print(f\"Procesando {len(video_paths)} videos del conjunto {subset}...\")\n",
    "\n",
    "    for video_path in tqdm(video_paths, desc=\"Extrayendo frames\"):\n",
    "        relative_path = video_path.relative_to(dataset_dir)\n",
    "        person_id = relative_path.parts[1]  # e.g. Train/person123\n",
    "        clip_id = relative_path.parts[2]    # clip_id (última carpeta antes del video)\n",
    "\n",
    "        if clip_id in valid_clip_ids:\n",
    "            # Genera el nuevo path: Train_Augmentation/person_id/clip_id/frames\n",
    "            output_folder = Path(output_base) / person_id / clip_id\n",
    "            output_folder.mkdir(parents=True, exist_ok=True)\n",
    "            extract_frames_from_video(video_path, output_folder, max_frames=max_frames_per_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9e283df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 5481 videos del conjunto Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo frames: 100%|██████████| 5481/5481 [00:05<00:00, 1043.77it/s]\n"
     ]
    }
   ],
   "source": [
    "getFramesPerVideo_filtered(\n",
    "    dataset_dir=\"../../Datasets/DaiSee/DAiSEE/DataSet/\",\n",
    "    valid_clip_ids=valid_clip_ids,\n",
    "    subset=\"Train\",\n",
    "    max_frames_per_video=5,  # o el número que necesites\n",
    "    output_base=\"../output_frames/Train_Augmentation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea87c4e9",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93edec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define las transformaciones\n",
    "augmentations = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Rotate(limit=10, p=0.5),\n",
    "    A.GaussianBlur(p=0.3),\n",
    "    A.HueSaturationValue(p=0.3),\n",
    "    A.RandomScale(scale_limit=0.1, p=0.5),\n",
    "    A.GaussNoise(p=0.3),\n",
    "])\n",
    "\n",
    "def augment_and_save_images(input_dir, output_dir, augment_times=5):\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        if img_name.lower().endswith(\".jpg\"):\n",
    "            img_path = input_dir / img_name\n",
    "            image = cv2.imread(str(img_path))\n",
    "\n",
    "            for i in range(augment_times):\n",
    "                augmented = augmentations(image=image)[\"image\"]\n",
    "                aug_filename = output_dir / f\"{img_name[:-4]}_aug{i}.jpg\"\n",
    "                cv2.imwrite(str(aug_filename), augmented)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "def load_labels(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    labels_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        # Convertir ClipID a str antes de usar replace y luego a int\n",
    "        clip_id = str(row['ClipID']).replace('.avi', '').replace('.mp4', '')\n",
    "        labels_dict[clip_id] = [\n",
    "            int(row['Boredom']),\n",
    "            int(row['Engagement']),\n",
    "            int(row['Confusion']),\n",
    "            int(row['Frustration'])\n",
    "        ]\n",
    "    return labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e07b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_facenet_embedding(img_path):\n",
    "    try:\n",
    "        embedding = DeepFace.represent(img_path=img_path, model_name='Facenet', enforce_detection=False)\n",
    "        return np.array(embedding[0]['embedding'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error en {img_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d5b9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(frames_root_folder, labels_csv, max_videos=1250):\n",
    "    labels = load_labels(labels_csv)\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Obtener todas las carpetas de video\n",
    "    frames_root_folder = Path(frames_root_folder)\n",
    "    video_folders = list(frames_root_folder.glob(\"*/*\"))\n",
    "    print(f\"Total de carpetas de video encontradas: {len(video_folders)}\")\n",
    "\n",
    "    # Limitar a los primeros `max_videos`\n",
    "    video_folders = video_folders[:max_videos]\n",
    "    print(f\"Procesando {len(video_folders)} carpetas de video...\")\n",
    "\n",
    "    # Procesar cada carpeta de video\n",
    "    for video_folder in tqdm(video_folders, desc=\"Procesando videos\"):\n",
    "        clip_id = video_folder.name\n",
    "\n",
    "        # Verificar si el clip_id está en labels\n",
    "        if clip_id not in labels:\n",
    "            with open(\"missing_clip_ids.txt\", \"a\") as f:\n",
    "                f.write(clip_id + \"\\n\")\n",
    "            print(f\"ClipID {clip_id} no encontrado en labels.\")\n",
    "            continue\n",
    "\n",
    "        emotion_levels = labels[clip_id]  # lista de 4 números\n",
    "\n",
    "        # Crear un vector one-hot de 16 posiciones\n",
    "        label_vector = np.zeros(16)\n",
    "        for i, level in enumerate(emotion_levels):  # i = 0 (boredom), 1 (engagement), 2 (confusion), 3 (frustration)\n",
    "            index = i * 4 + level  # cada emoción tiene 4 niveles\n",
    "            label_vector[index] = 1\n",
    "\n",
    "        video_embeddings = []\n",
    "        \n",
    "        # Procesar los frames dentro de esta carpeta\n",
    "        for frame_path in video_folder.glob(\"*.jpg\"):\n",
    "            embedding = extract_facenet_embedding(str(frame_path))\n",
    "            if embedding is not None:\n",
    "                video_embeddings.append(embedding)\n",
    "        \n",
    "        video_embedding = np.mean(video_embeddings, axis=0)\n",
    "        video_embedding = np.array(video_embedding)\n",
    "        \n",
    "        X.append(video_embedding)\n",
    "        y.append(label_vector)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Dataset procesado: {X.shape[0]} ejemplos.\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e88c7790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de carpetas de video encontradas: 248\n",
      "Procesando 248 carpetas de video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando videos:  10%|█         | 25/248 [00:44<06:37,  1.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_3796\\676562800.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Procesar train dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m frames_root = \u001b[33m\"../output_frames/Train_Augmentation\"\u001b[39m\n\u001b[32m      3\u001b[39m labels_csv = \u001b[33m\"../Labels/TrainLabels.csv\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m X_train_Augmentation, X_train_Augmentation = process_dataset(frames_root, labels_csv)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_3796\\3343498295.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(frames_root_folder, labels_csv, max_videos)\u001b[39m\n\u001b[32m     34\u001b[39m         video_embeddings = []\n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m         \u001b[38;5;66;03m# Procesar los frames dentro de esta carpeta\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m frame_path \u001b[38;5;28;01min\u001b[39;00m video_folder.glob(\u001b[33m\"*.jpg\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m             embedding = extract_facenet_embedding(str(frame_path))\n\u001b[32m     39\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m embedding \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     40\u001b[39m                 video_embeddings.append(embedding)\n\u001b[32m     41\u001b[39m \n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_3796\\3973586970.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(img_path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m extract_facenet_embedding(img_path):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      3\u001b[39m         embedding = DeepFace.represent(img_path=img_path, model_name=\u001b[33m'Facenet'\u001b[39m, enforce_detection=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array(embedding[\u001b[32m0\u001b[39m][\u001b[33m'embedding'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m      6\u001b[39m         print(\u001b[33mf\"Error en {img_path}: {e}\"\u001b[39m)\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\deepface\\DeepFace.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization, anti_spoofing, max_faces)\u001b[39m\n\u001b[32m    414\u001b[39m \n\u001b[32m    415\u001b[39m         - face_confidence (float): Confidence score of face detection. If `detector_backend` \u001b[38;5;28;01mis\u001b[39;00m set\n\u001b[32m    416\u001b[39m             to \u001b[33m'skip'\u001b[39m, the confidence will be \u001b[32m0\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mis\u001b[39;00m nonsensical.\n\u001b[32m    417\u001b[39m     \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     return representation.represent(\n\u001b[32m    419\u001b[39m         img_path=img_path,\n\u001b[32m    420\u001b[39m         model_name=model_name,\n\u001b[32m    421\u001b[39m         enforce_detection=enforce_detection,\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\deepface\\modules\\representation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(img_path, model_name, enforce_detection, detector_backend, align, expand_percentage, normalization, anti_spoofing, max_faces)\u001b[39m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m         \u001b[38;5;66;03m# custom normalization\u001b[39;00m\n\u001b[32m    131\u001b[39m         img = preprocessing.normalize_input(img=img, normalization=normalization)\n\u001b[32m    132\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m         embedding = model.forward(img)\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m         resp_objs.append(\n\u001b[32m    136\u001b[39m             {\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\deepface\\models\\FacialRecognition.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     25\u001b[39m                 \u001b[33mf\"but {self.model_name} not overwritten!\"\u001b[39m\n\u001b[32m     26\u001b[39m             )\n\u001b[32m     27\u001b[39m         \u001b[38;5;66;03m# model.predict causes memory issue when it is called in a for loop\u001b[39;00m\n\u001b[32m     28\u001b[39m         \u001b[38;5;66;03m# embedding = model.predict(img, verbose=0)[0].tolist()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.model(img, training=\u001b[38;5;28;01mFalse\u001b[39;00m).numpy()[\u001b[32m0\u001b[39m].tolist()\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     68\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m             \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m     70\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    584\u001b[39m                 super().__call__(inputs, *copied_args, **copied_kwargs)\n\u001b[32m    585\u001b[39m \n\u001b[32m    586\u001b[39m             layout_map_lib._map_subclass_model_variable(self, self._layout_map)\n\u001b[32m    587\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m super().__call__(*args, **kwargs)\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     68\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m             \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m     70\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1138\u001b[39m \n\u001b[32m   1139\u001b[39m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[32m   1140\u001b[39m                     self._compute_dtype_object\n\u001b[32m   1141\u001b[39m                 ):\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m                     outputs = call_fn(inputs, *args, **kwargs)\n\u001b[32m   1143\u001b[39m \n\u001b[32m   1144\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m self._activity_regularizer:\n\u001b[32m   1145\u001b[39m                     self._handle_activity_regularization(inputs, outputs)\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m                 new_e = e\n\u001b[32m    155\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m new_e.with_traceback(e.__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    156\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    157\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m signature\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m bound_signature\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, inputs, training, mask)\u001b[39m\n\u001b[32m    510\u001b[39m         Returns:\n\u001b[32m    511\u001b[39m             A tensor \u001b[38;5;28;01mif\u001b[39;00m there \u001b[38;5;28;01mis\u001b[39;00m a single output, \u001b[38;5;28;01mor\u001b[39;00m\n\u001b[32m    512\u001b[39m             a list of tensors \u001b[38;5;28;01mif\u001b[39;00m there are more than one outputs.\n\u001b[32m    513\u001b[39m         \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._run_internal_graph(inputs, training=training, mask=mask)\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, inputs, training, mask)\u001b[39m\n\u001b[32m    667\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m any(t_id \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m tensor_dict \u001b[38;5;28;01mfor\u001b[39;00m t_id \u001b[38;5;28;01min\u001b[39;00m node.flat_input_ids):\n\u001b[32m    668\u001b[39m                     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[32m    669\u001b[39m \n\u001b[32m    670\u001b[39m                 args, kwargs = node.map_arguments(tensor_dict)\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m                 outputs = node.layer(*args, **kwargs)\n\u001b[32m    672\u001b[39m \n\u001b[32m    673\u001b[39m                 \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[32m    674\u001b[39m                 for x_id, y in zip(\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     68\u001b[39m             \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m             \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m     70\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1138\u001b[39m \n\u001b[32m   1139\u001b[39m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[32m   1140\u001b[39m                     self._compute_dtype_object\n\u001b[32m   1141\u001b[39m                 ):\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m                     outputs = call_fn(inputs, *args, **kwargs)\n\u001b[32m   1143\u001b[39m \n\u001b[32m   1144\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m self._activity_regularizer:\n\u001b[32m   1145\u001b[39m                     self._handle_activity_regularization(inputs, outputs)\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m                 new_e = e\n\u001b[32m    155\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m new_e.with_traceback(e.__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    156\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    157\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m signature\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m             \u001b[38;5;28;01mdel\u001b[39;00m bound_signature\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\layers\\core\\activation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m call(self, inputs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.activation(inputs)\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\activations.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, alpha, max_value, threshold)\u001b[39m\n\u001b[32m    302\u001b[39m         A `Tensor` representing the input tensor, transformed by the relu\n\u001b[32m    303\u001b[39m         activation function. Tensor will be of the same shape \u001b[38;5;28;01mand\u001b[39;00m dtype of\n\u001b[32m    304\u001b[39m         input `x`.\n\u001b[32m    305\u001b[39m     \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     return backend.relu(\n\u001b[32m    307\u001b[39m         x, alpha=alpha, max_value=max_value, threshold=threshold\n\u001b[32m    308\u001b[39m     )\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1257\u001b[39m \n\u001b[32m   1258\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1262\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tf_keras\\src\\backend.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, alpha, max_value, threshold)\u001b[39m\n\u001b[32m   5393\u001b[39m         \u001b[38;5;66;03m# if no threshold, then can use nn.relu6 native TF op for performance\u001b[39;00m\n\u001b[32m   5394\u001b[39m         x = tf.nn.relu6(x)\n\u001b[32m   5395\u001b[39m         clip_max = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   5396\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5397\u001b[39m         x = tf.nn.relu(x)\n\u001b[32m   5398\u001b[39m \n\u001b[32m   5399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clip_max:\n\u001b[32m   5400\u001b[39m         max_value = _constant_to_tensor(max_value, x.dtype.base_dtype)\n",
      "\u001b[32md:\\OneDrive - Universidad Peruana de Ciencias\\Documents\\Carrera\\2025-1\\TP1\\code\\env\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(features, name)\u001b[39m\n\u001b[32m  11583\u001b[39m         _ctx, \"Relu\", name, features)\n\u001b[32m  11584\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m  11585\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m  11586\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m> \u001b[39m\u001b[32m11587\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m  11588\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m  11589\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m  11590\u001b[39m       _result = _dispatcher_for_relu(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Procesar train dataset\n",
    "frames_root = \"../output_frames/Train_Augmentation\"\n",
    "labels_csv = \"../Labels/TrainLabels.csv\" \n",
    "X_train_Augmentation, X_train_Augmentation = process_dataset(frames_root, labels_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
