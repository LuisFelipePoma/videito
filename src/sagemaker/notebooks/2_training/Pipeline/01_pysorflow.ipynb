{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6bc860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu126\n",
      "Torchvision version: 0.22.0+cu126\n",
      "✅ 1 GPU(s) detected:\n",
      "  - NVIDIA GeForce GTX 1650\n",
      "Current device: 0\n",
      "Default tensor type set to CUDA\n",
      "CUDA cache emptied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pepe/miniconda3/envs/torch/lib/python3.10/site-packages/torch/__init__.py:1240: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "torch.backends.cudnn.benchmark = True  \n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ {torch.cuda.device_count()} GPU(s) detected:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  - {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"❌ No GPU detected. Using CPU instead.\")\n",
    "\n",
    "# Set default tensor type to CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    print(\"Default tensor type set to CUDA\")\n",
    "    \n",
    "    # Try emptying cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache emptied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbfa882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaders.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dfa1474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: ../../DAiSEE/DataSet/Aug/0/train\n",
      "  → 989 videos found for class 0\n",
      "Processing folder: ../../DAiSEE/DataSet/Aug/1/train\n",
      "  → 1908 videos found for class 1\n",
      "Processing folder: ../../DAiSEE/DataSet/Aug/2/train\n",
      "  → 1050 videos found for class 2\n",
      "Processing folder: ../../DAiSEE/DataSet/Aug/3/train\n",
      "  → 1050 videos found for class 3\n",
      "Total videos for train: 4997\n",
      "Processing folder: ../../DAiSEE/DataSet/Aug/0/validation\n",
      "  → 9 videos found for class 0\n",
      "Processing folder: ../../DAiSEE/DataSet/Aug/1/validation\n",
      "  → 68 videos found for class 1\n",
      "Processing folder: ../../DAiSEE/DataSet/Aug/2/validation\n",
      "  → 225 videos found for class 2\n",
      "Processing folder: ../../DAiSEE/DataSet/Aug/3/validation\n",
      "  → 225 videos found for class 3\n",
      "Total videos for validation: 527\n",
      "Processing folder: ../../DAiSEE/DataSet/Aug/0/test\n",
      "  → 9 videos found for class 0\n",
      "Processing folder: ../../DAiSEE/DataSet/Aug/1/test\n",
      "  → 69 videos found for class 1\n",
      "Processing folder: ../../DAiSEE/DataSet/Aug/2/test\n",
      "  → 225 videos found for class 2\n",
      "Processing folder: ../../DAiSEE/DataSet/Aug/3/test\n",
      "  → 225 videos found for class 3\n",
      "Total videos for test: 528\n",
      "VALIDATING DATALOADERS\n",
      "-----------------\n",
      "Validating dataloader 'Training'...\n",
      "  ✅ Dataloader 'Training' is valid - batch shape: torch.Size([8, 16, 3, 124, 124])\n",
      "     Labels: tensor([0, 3, 1, 0, 0, 0, 2, 3])\n",
      "Validating dataloader 'Validation'...\n",
      "  ✅ Dataloader 'Validation' is valid - batch shape: torch.Size([8, 16, 3, 124, 124])\n",
      "     Labels: tensor([2, 3, 2, 2, 3, 2, 3, 1])\n",
      "Validating dataloader 'Test'...\n",
      "  ✅ Dataloader 'Test' is valid - batch shape: torch.Size([8, 16, 3, 124, 124])\n",
      "     Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "-----------------\n",
      "\n",
      "Checking data directory: ../../DAiSEE/DataSet/Aug\n",
      "✅ Directory exists: ../../DAiSEE/DataSet/Aug\n",
      "✅ Class directory 0 exists\n",
      "   - train: 989 video files\n",
      "     Example: aug0001-3100821052.avi (0.45 MB)\n",
      "   - validation: 9 video files\n",
      "     Example: 4100281067.avi (1.50 MB)\n",
      "   - test: 9 video files\n",
      "     Example: 2100582055.avi (1.30 MB)\n",
      "✅ Class directory 1 exists\n",
      "   - train: 1908 video files\n",
      "     Example: aug0003-4110311045.avi (0.44 MB)\n",
      "   - validation: 68 video files\n",
      "     Example: 3100641023.avi (1.56 MB)\n",
      "   - test: 69 video files\n",
      "     Example: 2026140257.mp4 (8.30 MB)\n",
      "✅ Class directory 2 exists\n",
      "   - train: 1050 video files\n",
      "     Example: 4000222044.avi (0.95 MB)\n",
      "   - validation: 225 video files\n",
      "     Example: 2100612033.avi (1.47 MB)\n",
      "   - test: 225 video files\n",
      "     Example: 3100711009.avi (1.88 MB)\n",
      "✅ Class directory 3 exists\n",
      "   - train: 1050 video files\n",
      "     Example: 7698620123.avi (3.02 MB)\n",
      "   - validation: 225 video files\n",
      "     Example: 2100591008.avi (1.10 MB)\n",
      "   - test: 225 video files\n",
      "     Example: 2100582064.avi (1.29 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and validate dataloaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(ROOT_DIR)\n",
    "\n",
    "# Improved validate_dataloader function with better error handling\n",
    "def validate_dataloader(loader, name):\n",
    "    print(f\"Validating dataloader '{name}'...\")\n",
    "    \n",
    "    # First check if dataset is empty\n",
    "    if len(loader.dataset) == 0:\n",
    "        print(f\"  ⚠️ Dataloader '{name}' is empty (contains no data)\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Try to get the first batch\n",
    "        iterator = iter(loader)\n",
    "        batch = next(iterator)\n",
    "        frames, labels = batch\n",
    "        print(f\"  ✅ Dataloader '{name}' is valid - batch shape: {frames.shape}\")\n",
    "        print(f\"     Labels: {labels}\")\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        print(f\"  ⚠️ Dataloader '{name}' is empty (no batches)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error getting first batch: {e}\")\n",
    "        print(f\"  Error type: {type(e).__name__}\")\n",
    "        return False\n",
    "\n",
    "# Execute validation with detailed feedback\n",
    "print(\"VALIDATING DATALOADERS\")\n",
    "print(\"-----------------\")\n",
    "valid_train = validate_dataloader(train_loader, \"Training\")\n",
    "valid_val = validate_dataloader(val_loader, \"Validation\")\n",
    "valid_test = validate_dataloader(test_loader, \"Test\")\n",
    "print(\"-----------------\")\n",
    "\n",
    "# Check dataset access permissions and files\n",
    "def check_dataset_path(root_dir):\n",
    "    print(f\"\\nChecking data directory: {root_dir}\")\n",
    "    \n",
    "    if not os.path.exists(root_dir):\n",
    "        print(f\"❌ ERROR: Directory does not exist: {root_dir}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"✅ Directory exists: {root_dir}\")\n",
    "    \n",
    "    # Check for class directories\n",
    "    for cls in CLASSES:\n",
    "        cls_dir = os.path.join(root_dir, str(cls))\n",
    "        if not os.path.exists(cls_dir):\n",
    "            print(f\"❌ ERROR: Class directory {cls} does not exist: {cls_dir}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"✅ Class directory {cls} exists\")\n",
    "        \n",
    "        # Check split directories\n",
    "        for split in SPLITS:\n",
    "            split_dir = os.path.join(cls_dir, split)\n",
    "            if not os.path.exists(split_dir):\n",
    "                print(f\"❌ ERROR: Split directory {split} does not exist: {split_dir}\")\n",
    "                continue\n",
    "                \n",
    "            # Count video files\n",
    "            video_files = [f for f in os.listdir(split_dir) \n",
    "                          if f.lower().endswith(('.mp4', '.avi'))]\n",
    "            print(f\"   - {split}: {len(video_files)} video files\")\n",
    "            \n",
    "            # Check file sizes\n",
    "            if video_files:\n",
    "                sample_file = os.path.join(split_dir, video_files[0])\n",
    "                size_mb = os.path.getsize(sample_file) / (1024 * 1024)\n",
    "                print(f\"     Example: {video_files[0]} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Check dataset structure\n",
    "check_dataset_path(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764caccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1,199,780\n",
      "Trainable parameters: 1,199,780\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "\n",
    "\n",
    "class VideoRNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=len(CLASSES),\n",
    "        feature_dim=128,\n",
    "        num_layers=2,\n",
    "        dropout=0.1,\n",
    "        rnn_type='gru'        # 'gru' o 'lstm'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Backbone EfficientNet-B0\n",
    "        weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
    "        base    = mobilenet_v3_small(weights=weights)\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            *list(base.features),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # Proyección a feature_dim\n",
    "        self.feature_reduce = nn.Linear(576, feature_dim)\n",
    "        self.layer_norm     = nn.LayerNorm(feature_dim)\n",
    "        self.dropout_in     = nn.Dropout(dropout)\n",
    "        # RNN\n",
    "        if rnn_type.lower() == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                feature_dim, feature_dim, num_layers=num_layers,\n",
    "                batch_first=True, dropout=dropout\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = nn.GRU(\n",
    "                feature_dim, feature_dim, num_layers=num_layers,\n",
    "                batch_first=True, dropout=dropout\n",
    "            )\n",
    "        # Clasificación\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(feature_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C, H, W]\n",
    "        B, T, C, H, W = x.shape\n",
    "        # 1) CNN por frame\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.feature_reduce(x)\n",
    "        # 2) Prepara secuencia\n",
    "        x = x.view(B, T, -1)             # [B, T, feature_dim]\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout_in(x)\n",
    "        # 3) RNN\n",
    "        # out: [B, T, feature_dim], h_n: [num_layers, B, feature_dim]\n",
    "        out, h_n = self.rnn(x)\n",
    "        # usa última salida de la capa superior\n",
    "        if isinstance(h_n, tuple):      # LSTM devuelve (h_n, c_n)\n",
    "            h_n = h_n[0]\n",
    "        last = h_n[-1]                  # [B, feature_dim]\n",
    "        # 4) Clasificación\n",
    "        logits = self.classifier(last)  # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "# Reemplazar instanciación:\n",
    "model = VideoRNNModel(rnn_type='gru').to(device)\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f3f189",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._dynamo' has no attribute 'config' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define loss function and optimizer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Añade después de importar torch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/adam.py:100\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor betas[1] must be 1-element\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     88\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     89\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     decoupled_weight_decay\u001b[38;5;241m=\u001b[39mdecoupled_weight_decay,\n\u001b[1;32m     99\u001b[0m )\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py:369\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    366\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/_compile.py:46\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dynamo_disable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     49\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/_dynamo/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:57\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m guard_bool\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_method\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     58\u001b[0m     config,\n\u001b[1;32m     59\u001b[0m     exc,\n\u001b[1;32m     60\u001b[0m     graph_break_hints,\n\u001b[1;32m     61\u001b[0m     logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging,\n\u001b[1;32m     62\u001b[0m     trace_rules,\n\u001b[1;32m     63\u001b[0m     variables,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_analysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     66\u001b[0m     get_indexof,\n\u001b[1;32m     67\u001b[0m     JUMP_OPNAMES,\n\u001b[1;32m     68\u001b[0m     livevars_analysis,\n\u001b[1;32m     69\u001b[0m     propagate_line_nums,\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     72\u001b[0m     cleaned_instructions,\n\u001b[1;32m     73\u001b[0m     create_call_function,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     unique_id,\n\u001b[1;32m     81\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:3260\u001b[0m\n\u001b[1;32m   3247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m   3248\u001b[0m     LEGACY_MOD_INLINELIST \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed.tensor._api\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3250\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed.tensor.device_mesh\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3258\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed._composable.replicate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3259\u001b[0m     }\n\u001b[0;32m-> 3260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mskip_fsdp_hooks:\n\u001b[1;32m   3261\u001b[0m         LEGACY_MOD_INLINELIST\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed.fsdp._fully_shard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3263\u001b[0m \u001b[38;5;66;03m# Force inline functions under these modules, even they are in *_SKIPLIST.\u001b[39;00m\n\u001b[1;32m   3264\u001b[0m \u001b[38;5;66;03m# We are using python module name instead of file or directory object to avoid circular dependency.\u001b[39;00m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;66;03m# Please keep this sorted alphabetically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3269\u001b[0m \u001b[38;5;66;03m# any *_INLINELIST or *_SKIPLIST: then, the behavior of Dynamo is that\u001b[39;00m\n\u001b[1;32m   3270\u001b[0m \u001b[38;5;66;03m# it will always inline into functions in the module.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch._dynamo' has no attribute 'config' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay  = 1e-5\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Añade después de importar torch\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# 1) Crea el scaler una vez\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Standard precision forward/backward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = targets.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        total       += batch_size\n",
    "        _, preds    = outputs.max(1)\n",
    "        correct    += preds.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    acc      = 100.0 * correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100.0 * correct / total\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return avg_loss, acc\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "\n",
    "try:\n",
    "    # Training parameters\n",
    "    num_epochs = 30\n",
    "    \n",
    "    # History for plotting\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Start training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "    print(\"✅ Training completed\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during training: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train')\n",
    "plt.plot(history['val_acc'], label='Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test on the test set\n",
    "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# 3.2 Recolecta predicciones y etiquetas verdaderas\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = outputs.max(1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(targets.cpu().tolist())\n",
    "\n",
    "# 3.3 Construye y muestra la matriz\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=CLASSES)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=CLASSES)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "disp.plot(ax=plt.gca(), cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
